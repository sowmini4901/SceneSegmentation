{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sowmini4901/SceneSegmentation/blob/main/SceneSegmentation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "otLCawpn72JF",
        "outputId": "72f4858e-e301-48db-9ab4-a7da86e3422a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting focal-loss\n",
            "  Downloading focal_loss-0.0.7-py3-none-any.whl (19 kB)\n",
            "Requirement already satisfied: tensorflow>=2.2 in /usr/local/lib/python3.7/dist-packages (from focal-loss) (2.7.0)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.2->focal-loss) (0.22.0)\n",
            "Requirement already satisfied: numpy>=1.14.5 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.2->focal-loss) (1.19.5)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.2->focal-loss) (1.6.3)\n",
            "Requirement already satisfied: libclang>=9.0.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.2->focal-loss) (12.0.0)\n",
            "Requirement already satisfied: absl-py>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.2->focal-loss) (0.12.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.2->focal-loss) (3.1.0)\n",
            "Requirement already satisfied: keras<2.8,>=2.7.0rc0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.2->focal-loss) (2.7.0)\n",
            "Requirement already satisfied: protobuf>=3.9.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.2->focal-loss) (3.17.3)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.2->focal-loss) (1.13.3)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.2->focal-loss) (3.10.0.2)\n",
            "Requirement already satisfied: keras-preprocessing>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.2->focal-loss) (1.1.2)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.2->focal-loss) (3.3.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.2->focal-loss) (1.15.0)\n",
            "Requirement already satisfied: tensorflow-estimator<2.8,~=2.7.0rc0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.2->focal-loss) (2.7.0)\n",
            "Requirement already satisfied: gast<0.5.0,>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.2->focal-loss) (0.4.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.2->focal-loss) (1.42.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.32.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.2->focal-loss) (0.37.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.2->focal-loss) (1.1.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.2->focal-loss) (0.2.0)\n",
            "Requirement already satisfied: tensorboard~=2.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.2->focal-loss) (2.7.0)\n",
            "Requirement already satisfied: flatbuffers<3.0,>=1.12 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.2->focal-loss) (2.0)\n",
            "Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py>=2.9.0->tensorflow>=2.2->focal-loss) (1.5.2)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow>=2.2->focal-loss) (1.35.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow>=2.2->focal-loss) (0.6.1)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow>=2.2->focal-loss) (57.4.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow>=2.2->focal-loss) (0.4.6)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow>=2.2->focal-loss) (2.23.0)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow>=2.2->focal-loss) (1.8.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow>=2.2->focal-loss) (3.3.6)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow>=2.2->focal-loss) (1.0.1)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard~=2.6->tensorflow>=2.2->focal-loss) (4.2.4)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard~=2.6->tensorflow>=2.2->focal-loss) (0.2.8)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard~=2.6->tensorflow>=2.2->focal-loss) (4.8)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.6->tensorflow>=2.2->focal-loss) (1.3.0)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard~=2.6->tensorflow>=2.2->focal-loss) (4.8.2)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard~=2.6->tensorflow>=2.2->focal-loss) (3.6.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard~=2.6->tensorflow>=2.2->focal-loss) (0.4.8)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow>=2.2->focal-loss) (2021.10.8)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow>=2.2->focal-loss) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow>=2.2->focal-loss) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow>=2.2->focal-loss) (3.0.4)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.6->tensorflow>=2.2->focal-loss) (3.1.1)\n",
            "Installing collected packages: focal-loss\n",
            "Successfully installed focal-loss-0.0.7\n"
          ]
        }
      ],
      "source": [
        "pip install focal-loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9ZsY0feuey5r",
        "outputId": "21e3b768-992b-4370-a6cb-8e1244ab4f0e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.7/dist-packages (2.7.0)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.13.3)\n",
            "Requirement already satisfied: flatbuffers<3.0,>=1.12 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (2.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.32.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (0.37.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (3.3.0)\n",
            "Requirement already satisfied: protobuf>=3.9.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (3.17.3)\n",
            "Requirement already satisfied: libclang>=9.0.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (12.0.0)\n",
            "Requirement already satisfied: keras-preprocessing>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.1.2)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (3.10.0.2)\n",
            "Requirement already satisfied: gast<0.5.0,>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (0.4.0)\n",
            "Requirement already satisfied: numpy>=1.14.5 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.19.5)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.15.0)\n",
            "Requirement already satisfied: absl-py>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (0.12.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (3.1.0)\n",
            "Requirement already satisfied: tensorflow-estimator<2.8,~=2.7.0rc0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (2.7.0)\n",
            "Requirement already satisfied: keras<2.8,>=2.7.0rc0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (2.7.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.1.0)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (0.22.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.42.0)\n",
            "Requirement already satisfied: tensorboard~=2.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (2.7.0)\n",
            "Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py>=2.9.0->tensorflow) (1.5.2)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow) (1.35.0)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow) (2.23.0)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow) (57.4.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow) (0.6.1)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow) (1.0.1)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow) (0.4.6)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow) (1.8.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow) (3.3.6)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard~=2.6->tensorflow) (4.8)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard~=2.6->tensorflow) (0.2.8)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard~=2.6->tensorflow) (4.2.4)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.6->tensorflow) (1.3.0)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard~=2.6->tensorflow) (4.8.2)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard~=2.6->tensorflow) (3.6.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard~=2.6->tensorflow) (0.4.8)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow) (2021.10.8)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.6->tensorflow) (3.1.1)\n"
          ]
        }
      ],
      "source": [
        "pip install tensorflow"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tu7Eumf4Hph6"
      },
      "outputs": [],
      "source": [
        "#importing necessary libraries for project\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "import numpy as np\n",
        "import sklearn\n",
        "import os\n",
        "import pickle\n",
        "import torch\n",
        "from sklearn import preprocessing\n",
        "from focal_loss import BinaryFocalLoss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O80_h03q70SZ",
        "outputId": "aa4456fa-9e1e-4edb-9daa-1b73ddb1be81"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ],
      "source": [
        "#mounting google drive for accessing files in it\n",
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/gdrive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hYwuuYhc7UD_"
      },
      "outputs": [],
      "source": [
        "#path for dataset\n",
        "folder='/content/gdrive/MyDrive/movies/data'\n",
        "files=os.listdir(folder)\n",
        "noOfFiles = len(files) \n",
        "shotsforMovie = np.zeros((noOfFiles), dtype = np.uint16)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lDKey4dtP7OP"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lKWsZKYUdZLS"
      },
      "outputs": [],
      "source": [
        "#initializing the variables needed\n",
        "k=0\n",
        "j=0\n",
        "counter=0\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "t9UI5heW6JK7",
        "outputId": "92f7d11f-0c2e-4ac4-e29a-650bdf0781a1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(25558, 7168)\n",
            "(25558,)\n",
            "(None, 2048, 1)\n",
            "(None, 2048, 1)\n",
            "(None, 2048, 1)\n",
            "(None, 2048, 1)\n",
            "(None, 2048, 1)\n",
            "(None, 2048, 1)\n",
            "(None, 2048, 4)\n",
            "(None, 2048, 4)\n",
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " reshape_8 (Reshape)         (None, 1, 2048, 4)        0         \n",
            "                                                                 \n",
            " conv2d (Conv2D)             (None, 1, 2048, 64)       832       \n",
            "                                                                 \n",
            " batch_normalization (BatchN  (None, 1, 2048, 64)      256       \n",
            " ormalization)                                                   \n",
            "                                                                 \n",
            " re_lu (ReLU)                (None, 1, 2048, 64)       0         \n",
            "                                                                 \n",
            " conv2d_1 (Conv2D)           (None, 1, 2048, 64)       12352     \n",
            "                                                                 \n",
            " batch_normalization_1 (Batc  (None, 1, 2048, 64)      256       \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " re_lu_1 (ReLU)              (None, 1, 2048, 64)       0         \n",
            "                                                                 \n",
            " max_pooling2d (MaxPooling2D  (None, 1, 1024, 64)      0         \n",
            " )                                                               \n",
            "                                                                 \n",
            " conv2d_2 (Conv2D)           (None, 1, 1024, 128)      24704     \n",
            "                                                                 \n",
            " batch_normalization_2 (Batc  (None, 1, 1024, 128)     512       \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " re_lu_2 (ReLU)              (None, 1, 1024, 128)      0         \n",
            "                                                                 \n",
            " conv2d_3 (Conv2D)           (None, 1, 1024, 128)      49280     \n",
            "                                                                 \n",
            " batch_normalization_3 (Batc  (None, 1, 1024, 128)     512       \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " re_lu_3 (ReLU)              (None, 1, 1024, 128)      0         \n",
            "                                                                 \n",
            " max_pooling2d_1 (MaxPooling  (None, 1, 512, 128)      0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " conv2d_4 (Conv2D)           (None, 1, 512, 256)       98560     \n",
            "                                                                 \n",
            " batch_normalization_4 (Batc  (None, 1, 512, 256)      1024      \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " re_lu_4 (ReLU)              (None, 1, 512, 256)       0         \n",
            "                                                                 \n",
            " conv2d_5 (Conv2D)           (None, 1, 512, 256)       196864    \n",
            "                                                                 \n",
            " batch_normalization_5 (Batc  (None, 1, 512, 256)      1024      \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " re_lu_5 (ReLU)              (None, 1, 512, 256)       0         \n",
            "                                                                 \n",
            " conv2d_6 (Conv2D)           (None, 1, 512, 256)       196864    \n",
            "                                                                 \n",
            " batch_normalization_6 (Batc  (None, 1, 512, 256)      1024      \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " re_lu_6 (ReLU)              (None, 1, 512, 256)       0         \n",
            "                                                                 \n",
            " max_pooling2d_2 (MaxPooling  (None, 1, 256, 256)      0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " conv2d_7 (Conv2D)           (None, 1, 256, 512)       393728    \n",
            "                                                                 \n",
            " batch_normalization_7 (Batc  (None, 1, 256, 512)      2048      \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " re_lu_7 (ReLU)              (None, 1, 256, 512)       0         \n",
            "                                                                 \n",
            " conv2d_8 (Conv2D)           (None, 1, 256, 512)       786944    \n",
            "                                                                 \n",
            " batch_normalization_8 (Batc  (None, 1, 256, 512)      2048      \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " re_lu_8 (ReLU)              (None, 1, 256, 512)       0         \n",
            "                                                                 \n",
            " conv2d_9 (Conv2D)           (None, 1, 256, 512)       786944    \n",
            "                                                                 \n",
            " batch_normalization_9 (Batc  (None, 1, 256, 512)      2048      \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " re_lu_9 (ReLU)              (None, 1, 256, 512)       0         \n",
            "                                                                 \n",
            " max_pooling2d_3 (MaxPooling  (None, 1, 128, 512)      0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " conv2d_10 (Conv2D)          (None, 1, 128, 512)       786944    \n",
            "                                                                 \n",
            " batch_normalization_10 (Bat  (None, 1, 128, 512)      2048      \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " re_lu_10 (ReLU)             (None, 1, 128, 512)       0         \n",
            "                                                                 \n",
            " conv2d_11 (Conv2D)          (None, 1, 128, 512)       786944    \n",
            "                                                                 \n",
            " batch_normalization_11 (Bat  (None, 1, 128, 512)      2048      \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " re_lu_11 (ReLU)             (None, 1, 128, 512)       0         \n",
            "                                                                 \n",
            " conv2d_12 (Conv2D)          (None, 1, 128, 512)       786944    \n",
            "                                                                 \n",
            " batch_normalization_12 (Bat  (None, 1, 128, 512)      2048      \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " re_lu_12 (ReLU)             (None, 1, 128, 512)       0         \n",
            "                                                                 \n",
            " max_pooling2d_4 (MaxPooling  (None, 1, 64, 512)       0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " max_pooling2d_5 (MaxPooling  (None, 1, 32, 512)       0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " conv2d_13 (Conv2D)          (None, 1, 32, 16)         8208      \n",
            "                                                                 \n",
            " batch_normalization_13 (Bat  (None, 1, 32, 16)        64        \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " re_lu_13 (ReLU)             (None, 1, 32, 16)         0         \n",
            "                                                                 \n",
            " conv2d_14 (Conv2D)          (None, 1, 32, 64)         3136      \n",
            "                                                                 \n",
            " batch_normalization_14 (Bat  (None, 1, 32, 64)        256       \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " re_lu_14 (ReLU)             (None, 1, 32, 64)         0         \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 4,936,464\n",
            "Trainable params: 4,927,856\n",
            "Non-trainable params: 8,608\n",
            "_________________________________________________________________\n",
            "None\n",
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " reshape_9 (Reshape)         (None, 1, 2048, 4)        0         \n",
            "                                                                 \n",
            " conv2d_15 (Conv2D)          (None, 1, 2048, 64)       832       \n",
            "                                                                 \n",
            " batch_normalization_15 (Bat  (None, 1, 2048, 64)      256       \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " re_lu_15 (ReLU)             (None, 1, 2048, 64)       0         \n",
            "                                                                 \n",
            " conv2d_16 (Conv2D)          (None, 1, 2048, 64)       12352     \n",
            "                                                                 \n",
            " batch_normalization_16 (Bat  (None, 1, 2048, 64)      256       \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " re_lu_16 (ReLU)             (None, 1, 2048, 64)       0         \n",
            "                                                                 \n",
            " max_pooling2d_6 (MaxPooling  (None, 1, 1024, 64)      0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " conv2d_17 (Conv2D)          (None, 1, 1024, 128)      24704     \n",
            "                                                                 \n",
            " batch_normalization_17 (Bat  (None, 1, 1024, 128)     512       \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " re_lu_17 (ReLU)             (None, 1, 1024, 128)      0         \n",
            "                                                                 \n",
            " conv2d_18 (Conv2D)          (None, 1, 1024, 128)      49280     \n",
            "                                                                 \n",
            " batch_normalization_18 (Bat  (None, 1, 1024, 128)     512       \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " re_lu_18 (ReLU)             (None, 1, 1024, 128)      0         \n",
            "                                                                 \n",
            " max_pooling2d_7 (MaxPooling  (None, 1, 512, 128)      0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " conv2d_19 (Conv2D)          (None, 1, 512, 256)       98560     \n",
            "                                                                 \n",
            " batch_normalization_19 (Bat  (None, 1, 512, 256)      1024      \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " re_lu_19 (ReLU)             (None, 1, 512, 256)       0         \n",
            "                                                                 \n",
            " conv2d_20 (Conv2D)          (None, 1, 512, 256)       196864    \n",
            "                                                                 \n",
            " batch_normalization_20 (Bat  (None, 1, 512, 256)      1024      \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " re_lu_20 (ReLU)             (None, 1, 512, 256)       0         \n",
            "                                                                 \n",
            " conv2d_21 (Conv2D)          (None, 1, 512, 256)       196864    \n",
            "                                                                 \n",
            " batch_normalization_21 (Bat  (None, 1, 512, 256)      1024      \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " re_lu_21 (ReLU)             (None, 1, 512, 256)       0         \n",
            "                                                                 \n",
            " max_pooling2d_8 (MaxPooling  (None, 1, 256, 256)      0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " conv2d_22 (Conv2D)          (None, 1, 256, 512)       393728    \n",
            "                                                                 \n",
            " batch_normalization_22 (Bat  (None, 1, 256, 512)      2048      \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " re_lu_22 (ReLU)             (None, 1, 256, 512)       0         \n",
            "                                                                 \n",
            " conv2d_23 (Conv2D)          (None, 1, 256, 512)       786944    \n",
            "                                                                 \n",
            " batch_normalization_23 (Bat  (None, 1, 256, 512)      2048      \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " re_lu_23 (ReLU)             (None, 1, 256, 512)       0         \n",
            "                                                                 \n",
            " conv2d_24 (Conv2D)          (None, 1, 256, 512)       786944    \n",
            "                                                                 \n",
            " batch_normalization_24 (Bat  (None, 1, 256, 512)      2048      \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " re_lu_24 (ReLU)             (None, 1, 256, 512)       0         \n",
            "                                                                 \n",
            " max_pooling2d_9 (MaxPooling  (None, 1, 128, 512)      0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " conv2d_25 (Conv2D)          (None, 1, 128, 512)       786944    \n",
            "                                                                 \n",
            " batch_normalization_25 (Bat  (None, 1, 128, 512)      2048      \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " re_lu_25 (ReLU)             (None, 1, 128, 512)       0         \n",
            "                                                                 \n",
            " conv2d_26 (Conv2D)          (None, 1, 128, 512)       786944    \n",
            "                                                                 \n",
            " batch_normalization_26 (Bat  (None, 1, 128, 512)      2048      \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " re_lu_26 (ReLU)             (None, 1, 128, 512)       0         \n",
            "                                                                 \n",
            " conv2d_27 (Conv2D)          (None, 1, 128, 512)       786944    \n",
            "                                                                 \n",
            " batch_normalization_27 (Bat  (None, 1, 128, 512)      2048      \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " re_lu_27 (ReLU)             (None, 1, 128, 512)       0         \n",
            "                                                                 \n",
            " max_pooling2d_10 (MaxPoolin  (None, 1, 64, 512)       0         \n",
            " g2D)                                                            \n",
            "                                                                 \n",
            " max_pooling2d_11 (MaxPoolin  (None, 1, 32, 512)       0         \n",
            " g2D)                                                            \n",
            "                                                                 \n",
            " conv2d_28 (Conv2D)          (None, 1, 32, 16)         8208      \n",
            "                                                                 \n",
            " batch_normalization_28 (Bat  (None, 1, 32, 16)        64        \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " re_lu_28 (ReLU)             (None, 1, 32, 16)         0         \n",
            "                                                                 \n",
            " conv2d_29 (Conv2D)          (None, 1, 32, 64)         3136      \n",
            "                                                                 \n",
            " batch_normalization_29 (Bat  (None, 1, 32, 64)        256       \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " re_lu_29 (ReLU)             (None, 1, 32, 64)         0         \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 4,936,464\n",
            "Trainable params: 4,927,856\n",
            "Non-trainable params: 8,608\n",
            "_________________________________________________________________\n",
            "None\n",
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_1 (InputLayer)           [(None, 7168)]       0           []                               \n",
            "                                                                                                  \n",
            " tf.split (TFOpLambda)          [(None, 2048),       0           ['input_1[0][0]']                \n",
            "                                 (None, 512),                                                     \n",
            "                                 (None, 512),                                                     \n",
            "                                 (None, 512),                                                     \n",
            "                                 (None, 2048),                                                    \n",
            "                                 (None, 512),                                                     \n",
            "                                 (None, 512),                                                     \n",
            "                                 (None, 512)]                                                     \n",
            "                                                                                                  \n",
            " concatenate (Concatenate)      (None, 2048)         0           ['tf.split[0][1]',               \n",
            "                                                                  'tf.split[0][1]',               \n",
            "                                                                  'tf.split[0][1]',               \n",
            "                                                                  'tf.split[0][1]']               \n",
            "                                                                                                  \n",
            " concatenate_1 (Concatenate)    (None, 2048)         0           ['tf.split[0][2]',               \n",
            "                                                                  'tf.split[0][2]',               \n",
            "                                                                  'tf.split[0][2]',               \n",
            "                                                                  'tf.split[0][2]']               \n",
            "                                                                                                  \n",
            " concatenate_2 (Concatenate)    (None, 2048)         0           ['tf.split[0][3]',               \n",
            "                                                                  'tf.split[0][3]',               \n",
            "                                                                  'tf.split[0][3]',               \n",
            "                                                                  'tf.split[0][3]']               \n",
            "                                                                                                  \n",
            " concatenate_3 (Concatenate)    (None, 2048)         0           ['tf.split[0][5]',               \n",
            "                                                                  'tf.split[0][5]',               \n",
            "                                                                  'tf.split[0][5]',               \n",
            "                                                                  'tf.split[0][5]']               \n",
            "                                                                                                  \n",
            " concatenate_4 (Concatenate)    (None, 2048)         0           ['tf.split[0][6]',               \n",
            "                                                                  'tf.split[0][6]',               \n",
            "                                                                  'tf.split[0][6]',               \n",
            "                                                                  'tf.split[0][6]']               \n",
            "                                                                                                  \n",
            " concatenate_5 (Concatenate)    (None, 2048)         0           ['tf.split[0][7]',               \n",
            "                                                                  'tf.split[0][7]',               \n",
            "                                                                  'tf.split[0][7]',               \n",
            "                                                                  'tf.split[0][7]']               \n",
            "                                                                                                  \n",
            " reshape (Reshape)              (None, 2048, 1)      0           ['tf.split[0][0]']               \n",
            "                                                                                                  \n",
            " reshape_2 (Reshape)            (None, 2048, 1)      0           ['concatenate[0][0]']            \n",
            "                                                                                                  \n",
            " reshape_3 (Reshape)            (None, 2048, 1)      0           ['concatenate_1[0][0]']          \n",
            "                                                                                                  \n",
            " reshape_4 (Reshape)            (None, 2048, 1)      0           ['concatenate_2[0][0]']          \n",
            "                                                                                                  \n",
            " reshape_1 (Reshape)            (None, 2048, 1)      0           ['tf.split[0][4]']               \n",
            "                                                                                                  \n",
            " reshape_5 (Reshape)            (None, 2048, 1)      0           ['concatenate_3[0][0]']          \n",
            "                                                                                                  \n",
            " reshape_6 (Reshape)            (None, 2048, 1)      0           ['concatenate_4[0][0]']          \n",
            "                                                                                                  \n",
            " reshape_7 (Reshape)            (None, 2048, 1)      0           ['concatenate_5[0][0]']          \n",
            "                                                                                                  \n",
            " concatenate_6 (Concatenate)    (None, 2048, 4)      0           ['reshape[0][0]',                \n",
            "                                                                  'reshape_2[0][0]',              \n",
            "                                                                  'reshape_3[0][0]',              \n",
            "                                                                  'reshape_4[0][0]']              \n",
            "                                                                                                  \n",
            " concatenate_7 (Concatenate)    (None, 2048, 4)      0           ['reshape_1[0][0]',              \n",
            "                                                                  'reshape_5[0][0]',              \n",
            "                                                                  'reshape_6[0][0]',              \n",
            "                                                                  'reshape_7[0][0]']              \n",
            "                                                                                                  \n",
            " sequential (Sequential)        (None, 1, 32, 64)    4936464     ['concatenate_6[0][0]']          \n",
            "                                                                                                  \n",
            " sequential_1 (Sequential)      (None, 1, 32, 64)    4936464     ['concatenate_7[0][0]']          \n",
            "                                                                                                  \n",
            " concatenate_8 (Concatenate)    (None, 1, 64, 64)    0           ['sequential[0][0]',             \n",
            "                                                                  'sequential_1[0][0]']           \n",
            "                                                                                                  \n",
            " flatten (Flatten)              (None, 4096)         0           ['concatenate_8[0][0]']          \n",
            "                                                                                                  \n",
            " dropout (Dropout)              (None, 4096)         0           ['flatten[0][0]']                \n",
            "                                                                                                  \n",
            " dense (Dense)                  (None, 4096)         16781312    ['dropout[0][0]']                \n",
            "                                                                                                  \n",
            " dropout_1 (Dropout)            (None, 4096)         0           ['dense[0][0]']                  \n",
            "                                                                                                  \n",
            " dense_1 (Dense)                (None, 2048)         8390656     ['dropout_1[0][0]']              \n",
            "                                                                                                  \n",
            " dropout_2 (Dropout)            (None, 2048)         0           ['dense_1[0][0]']                \n",
            "                                                                                                  \n",
            " dense_2 (Dense)                (None, 2)            4098        ['dropout_2[0][0]']              \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 35,048,994\n",
            "Trainable params: 35,031,778\n",
            "Non-trainable params: 17,216\n",
            "__________________________________________________________________________________________________\n",
            "Epoch 1/16\n",
            "639/639 - 261s - loss: 50.7495 - accuracy: 0.9098 - val_loss: 44.1009 - val_accuracy: 0.9229 - 261s/epoch - 408ms/step\n",
            "Epoch 2/16\n",
            "639/639 - 244s - loss: 50.7935 - accuracy: 0.9112 - val_loss: 44.1009 - val_accuracy: 0.9229 - 244s/epoch - 382ms/step\n",
            "Epoch 3/16\n",
            "639/639 - 244s - loss: 50.7935 - accuracy: 0.9112 - val_loss: 44.1009 - val_accuracy: 0.9229 - 244s/epoch - 382ms/step\n",
            "Epoch 4/16\n",
            "639/639 - 244s - loss: 50.7935 - accuracy: 0.9112 - val_loss: 44.1009 - val_accuracy: 0.9229 - 244s/epoch - 381ms/step\n",
            "Epoch 5/16\n",
            "639/639 - 244s - loss: 50.7935 - accuracy: 0.9112 - val_loss: 44.1009 - val_accuracy: 0.9229 - 244s/epoch - 381ms/step\n",
            "Epoch 6/16\n",
            "639/639 - 242s - loss: 50.7935 - accuracy: 0.9112 - val_loss: 44.1009 - val_accuracy: 0.9229 - 242s/epoch - 379ms/step\n",
            "Epoch 7/16\n",
            "639/639 - 241s - loss: 50.7935 - accuracy: 0.9112 - val_loss: 44.1009 - val_accuracy: 0.9229 - 241s/epoch - 377ms/step\n",
            "Epoch 8/16\n",
            "639/639 - 241s - loss: 50.7936 - accuracy: 0.9112 - val_loss: 44.1009 - val_accuracy: 0.9229 - 241s/epoch - 377ms/step\n",
            "Epoch 9/16\n",
            "639/639 - 243s - loss: 50.7935 - accuracy: 0.9112 - val_loss: 44.1009 - val_accuracy: 0.9229 - 243s/epoch - 380ms/step\n",
            "Epoch 10/16\n",
            "639/639 - 243s - loss: 50.7935 - accuracy: 0.9112 - val_loss: 44.1009 - val_accuracy: 0.9229 - 243s/epoch - 380ms/step\n",
            "Epoch 11/16\n",
            "639/639 - 242s - loss: 50.7935 - accuracy: 0.9112 - val_loss: 44.1009 - val_accuracy: 0.9229 - 242s/epoch - 379ms/step\n",
            "Epoch 12/16\n",
            "639/639 - 242s - loss: 50.7935 - accuracy: 0.9112 - val_loss: 44.1009 - val_accuracy: 0.9229 - 242s/epoch - 379ms/step\n",
            "Epoch 13/16\n",
            "639/639 - 242s - loss: 50.7935 - accuracy: 0.9112 - val_loss: 44.1009 - val_accuracy: 0.9229 - 242s/epoch - 379ms/step\n",
            "Epoch 14/16\n",
            "639/639 - 241s - loss: 50.7935 - accuracy: 0.9112 - val_loss: 44.1009 - val_accuracy: 0.9229 - 241s/epoch - 377ms/step\n",
            "Epoch 15/16\n",
            "639/639 - 241s - loss: 50.7935 - accuracy: 0.9112 - val_loss: 44.1009 - val_accuracy: 0.9229 - 241s/epoch - 377ms/step\n",
            "Epoch 16/16\n",
            "639/639 - 241s - loss: 50.7935 - accuracy: 0.9112 - val_loss: 44.1009 - val_accuracy: 0.9229 - 241s/epoch - 377ms/step\n",
            "/content/gdrive/MyDrive/movies/data/softmax_1.h5\n"
          ]
        }
      ],
      "source": [
        "#going through each movie file\n",
        "for i in files:\n",
        "  filename=folder+'/'+i\n",
        "  with open(filename,'rb') as f:\n",
        "     data=pickle.load(f)\n",
        "  counter=counter+1\n",
        "  if counter==20:\n",
        "    break\n",
        "  #converting tensors to arrays for features\n",
        "  place=data['place']\n",
        "  place=place.data.numpy()\n",
        "  placeSize=place.shape[1]\n",
        "\n",
        "  cast=data['cast']\n",
        "  cast=cast.data.numpy()\n",
        "  castSize=cast.shape[1]\n",
        "\n",
        "  action=data['action']\n",
        "  action=action.data.numpy()\n",
        "  actionSize=action.shape[1]\n",
        "\n",
        "  audio=data['audio']\n",
        "  audio=audio.data.numpy()\n",
        "  audioSize=audio.shape[1]\n",
        "\n",
        "  #combining all the features into a huge table of 1101 rows(no.of shots) and 3584 columns(2048+512+512+512)\n",
        "  x = np.hstack((place, cast, action, audio))\n",
        "  y = data['scene_transition_boundary_ground_truth']\n",
        " \n",
        "  N = x.shape[0] \n",
        "  x_fold = np.zeros((N - 1, 2*x.shape[1])) \n",
        "\n",
        "  #adding the adjoining shots to abstract features from it.\n",
        "  for i in range(N - 1):\n",
        "    x_fold[i,:] = np.hstack((x[i,:], x[i+1,:])) \n",
        "    \n",
        "  if (j == 0):\n",
        "    X = x_fold\n",
        "    Y = y\n",
        "\n",
        "  else:\n",
        "    X = np.concatenate((X, x_fold), axis = 0)\n",
        "    Y = np.concatenate((Y, y))\n",
        "    \n",
        "\n",
        "    \n",
        "  j = j + 1\n",
        "  k = k + 1\n",
        "\n",
        "print(X.shape)\n",
        "print(Y.shape)\n",
        "\n",
        "#converting the predictions to 0 and 1 from false and true\n",
        "M = Y.shape[0]\n",
        "YTruth = np.zeros((M), dtype = np.uint8)\n",
        "for i in range(M):\n",
        "  if(Y[i] == True):\n",
        "      YTruth[i] = 1\n",
        "  elif(Y[i] == False):\n",
        "      YTruth[i] = 0\n",
        "\n",
        "# Create train and test datasets for cross-validation\n",
        "shotsdataset = np.zeros((noOfFiles), dtype = np.uint32)\n",
        "previous = 0\n",
        "for i in range(noOfFiles):\n",
        "  shotsforMovie[i] = shotsforMovie[i] - 1\n",
        "  shotsdataset[i] = shotsforMovie[i] + previous\n",
        "  previous = shotsdataset[i]\n",
        "  \n",
        "\n",
        "\n",
        "iterationsizes = np.zeros((4), dtype = np.uint32)\n",
        "iterationsizes[0] = 0.2*noOfFiles - 1\n",
        "iterationsizes[1] = 0.4*noOfFiles - 1\n",
        "iterationsizes[2] = 0.6*noOfFiles - 1\n",
        "iterationsizes[3] = 0.8*noOfFiles - 1\n",
        "  \n",
        "# Create IDs for cross-validation\n",
        "iter_ids = np.zeros((5,2), dtype = np.uint32)\n",
        "iter_ids[0,0] = 0 \n",
        "iter_ids[0,1] = shotsdataset[iterationsizes[0]]\n",
        "iter_ids[1,0] = shotsdataset[iterationsizes[0]]\n",
        "iter_ids[1,1] = shotsdataset[iterationsizes[1]]\n",
        "iter_ids[2,0] = shotsdataset[iterationsizes[1]]\n",
        "iter_ids[2,1] = shotsdataset[iterationsizes[2]]\n",
        "iter_ids[3,0] = shotsdataset[iterationsizes[2]]\n",
        "iter_ids[3,1] = shotsdataset[iterationsizes[3]]\n",
        "iter_ids[4,0] = shotsdataset[iterationsizes[3]]\n",
        "iter_ids[4,1] = M\n",
        "\n",
        "array_dims = np.zeros((8), dtype = np.int32)\n",
        "array_dims[0] = placeSize\n",
        "array_dims[1] = castSize\n",
        "array_dims[2] = actionSize\n",
        "array_dims[3] = audioSize\n",
        "array_dims[4] = array_dims[0]\n",
        "array_dims[5] = array_dims[1]\n",
        "array_dims[6] = array_dims[2]\n",
        "array_dims[7] = array_dims[3]\n",
        "\n",
        "inputs = tf.keras.layers.Input(shape = (X.shape[1]))\n",
        "x1, x2, x3, x4, x5, x6, x7, x8 = tf.split(inputs, array_dims, axis = 1)\n",
        "\n",
        "x1_r = tf.keras.layers.Reshape((placeSize, 1))(x1)\n",
        "#x1_b = tf.keras.layers.BatchNormalization(axis=1)(x1_r)\n",
        "\n",
        "x5_r = tf.keras.layers.Reshape((placeSize, 1))(x5)\n",
        "#x5_b = tf.keras.layers.BatchNormalization(axis=1)(x5_r)\n",
        "\n",
        "#x2b = tf.keras.layers.BatchNormalization(axis=1)(x2)\n",
        "x2_conc = tf.keras.layers.concatenate([x2, x2, x2, x2], axis = 1) # create vectors of size (None, 2048) from size (None, 512)\n",
        "x2_r = tf.keras.layers.Reshape((placeSize, 1))(x2_conc)\n",
        "print(x2_r.shape)\n",
        "\n",
        "#x3b = tf.keras.layers.BatchNormalization(axis=1)(x3)\n",
        "x3_conc = tf.keras.layers.concatenate([x3, x3, x3, x3], axis = 1)\n",
        "x3_r = tf.keras.layers.Reshape((placeSize, 1))(x3_conc)\n",
        "print(x3_r.shape)\n",
        "\n",
        "#x4b = tf.keras.layers.BatchNormalization(axis=1)(x4)\n",
        "x4_conc = tf.keras.layers.concatenate([x4, x4, x4, x4], axis = 1)\n",
        "x4_r = tf.keras.layers.Reshape((placeSize, 1))(x4_conc)\n",
        "print(x4_r.shape)\n",
        "\n",
        "#x6b = tf.keras.layers.BatchNormalization(axis=1)(x6)\n",
        "x6_conc = tf.keras.layers.concatenate([x6, x6, x6, x6], axis = 1) # create vectors of size (None, 2048) from size (None, 512)\n",
        "x6_r = tf.keras.layers.Reshape((placeSize, 1))(x6_conc)\n",
        "print(x6_r.shape)\n",
        "\n",
        "#x7b = tf.keras.layers.BatchNormalization(axis=1)(x7)\n",
        "x7_conc = tf.keras.layers.concatenate([x7, x7, x7, x7], axis = 1)\n",
        "x7_r = tf.keras.layers.Reshape((placeSize, 1))(x7_conc)\n",
        "print(x7_r.shape)\n",
        "\n",
        "#x8b = tf.keras.layers.BatchNormalization(axis=1)(x8)\n",
        "x8_conc = tf.keras.layers.concatenate([x8, x8, x8, x8], axis = 1)\n",
        "x8_r = tf.keras.layers.Reshape((placeSize, 1))(x8_conc)\n",
        "print(x8_r.shape)\n",
        "\n",
        "shot_1 = tf.keras.layers.concatenate([x1_r, x2_r, x3_r, x4_r], axis = 2) # create a vector of size (None, 2048, 4)\n",
        "print(shot_1.shape)\n",
        "\n",
        "shot_2 = tf.keras.layers.concatenate([x5_r, x6_r, x7_r, x8_r], axis = 2) # create a vector of size (None, 2048, 4)\n",
        "print(shot_2.shape)\n",
        "\n",
        "# building VGG net(standard architecture)\n",
        "conv1 = tf.keras.Sequential()\n",
        "conv1.add(tf.keras.layers.Reshape((1, placeSize, 4)))\n",
        "\n",
        "conv1.add(tf.keras.layers.Conv2D(filters=64, kernel_size=(1, 3), strides=1, padding='same', activation='linear')) #input size (None, 1, 2048, 4)\n",
        "conv1.add(tf.keras.layers.BatchNormalization(axis=3))\n",
        "conv1.add(tf.keras.layers.ReLU())\n",
        "\n",
        "conv1.add(tf.keras.layers.Conv2D(filters=64, kernel_size=(1, 3), strides=1, padding='same', activation='linear')) #input size (None, 1, 2048, 64)\n",
        "conv1.add(tf.keras.layers.BatchNormalization(axis=3))\n",
        "conv1.add(tf.keras.layers.ReLU())\n",
        "\n",
        "conv1.add(tf.keras.layers.MaxPool2D(pool_size=(1,2),strides=(2,2)))  #input size(none,1,2048,64)\n",
        "\n",
        "\n",
        "conv1.add(tf.keras.layers.Conv2D(filters=128, kernel_size=(1, 3), strides=1, padding='same', activation='linear')) #input size (None, 1, 1024, 64)\n",
        "conv1.add(tf.keras.layers.BatchNormalization(axis=3))\n",
        "conv1.add(tf.keras.layers.ReLU())\n",
        "\n",
        "conv1.add(tf.keras.layers.Conv2D(filters=128, kernel_size=(1, 3), strides=1, padding='same', activation='linear')) #input size (None, 1, 1024,64)\n",
        "conv1.add(tf.keras.layers.BatchNormalization(axis=3))\n",
        "conv1.add(tf.keras.layers.ReLU())\n",
        "\n",
        "conv1.add(tf.keras.layers.MaxPool2D(pool_size=(1,2),strides=(2,2)))   #input size(none,1,512,128)\n",
        "\n",
        "\n",
        "conv1.add(tf.keras.layers.Conv2D(filters=256, kernel_size=(1, 3), strides=1, padding='same', activation='linear')) #input size (None, 1, 512, 128)\n",
        "conv1.add(tf.keras.layers.BatchNormalization(axis=3))\n",
        "conv1.add(tf.keras.layers.ReLU())\n",
        "\n",
        "conv1.add(tf.keras.layers.Conv2D(filters=256, kernel_size=(1, 3), strides=1, padding='same', activation='linear')) #input size (None, 1, 512, 128)\n",
        "conv1.add(tf.keras.layers.BatchNormalization(axis=3))\n",
        "conv1.add(tf.keras.layers.ReLU())\n",
        "\n",
        "conv1.add(tf.keras.layers.Conv2D(filters=256, kernel_size=(1, 3), strides=1, padding='same', activation='linear')) #input size (None, 1, 512, 128)\n",
        "conv1.add(tf.keras.layers.BatchNormalization(axis=3))\n",
        "conv1.add(tf.keras.layers.ReLU())\n",
        "\n",
        "conv1.add(tf.keras.layers.MaxPool2D(pool_size=(1,2),strides=(2,2))) #output size (None, 1, 256, 256)\n",
        "\n",
        "conv1.add(tf.keras.layers.Conv2D(filters=512, kernel_size=(1, 3), strides=1, padding='same', activation='linear')) #input size (None, 1, 256, 256)\n",
        "conv1.add(tf.keras.layers.BatchNormalization(axis=3))\n",
        "conv1.add(tf.keras.layers.ReLU())\n",
        "\n",
        "\n",
        "conv1.add(tf.keras.layers.Conv2D(filters=512, kernel_size=(1, 3), strides=1, padding='same', activation='linear')) #input size (None, 1, 256, 256)\n",
        "conv1.add(tf.keras.layers.BatchNormalization(axis=3))\n",
        "conv1.add(tf.keras.layers.ReLU())\n",
        "\n",
        "conv1.add(tf.keras.layers.Conv2D(filters=512, kernel_size=(1, 3), strides=1, padding='same', activation='linear')) #input size (None, 1, 256, 256)\n",
        "conv1.add(tf.keras.layers.BatchNormalization(axis=3))\n",
        "conv1.add(tf.keras.layers.ReLU())\n",
        "\n",
        "conv1.add(tf.keras.layers.MaxPool2D(pool_size=(1,2),strides=(2,2))) #output size (None, 1, 128, 512)\n",
        "\n",
        "conv1.add(tf.keras.layers.Conv2D(filters=512, kernel_size=(1, 3), strides=1, padding='same', activation='linear')) #input size (None, 1, 128, 512)\n",
        "conv1.add(tf.keras.layers.BatchNormalization(axis=3))\n",
        "conv1.add(tf.keras.layers.ReLU())\n",
        "\n",
        "\n",
        "conv1.add(tf.keras.layers.Conv2D(filters=512, kernel_size=(1, 3), strides=1, padding='same', activation='linear')) #input size (None, 1, 128, 512)\n",
        "conv1.add(tf.keras.layers.BatchNormalization(axis=3))\n",
        "conv1.add(tf.keras.layers.ReLU())\n",
        "\n",
        "conv1.add(tf.keras.layers.Conv2D(filters=512, kernel_size=(1, 3), strides=1, padding='same', activation='linear')) #input size (None, 1, 128, 512)\n",
        "conv1.add(tf.keras.layers.BatchNormalization(axis=3))\n",
        "conv1.add(tf.keras.layers.ReLU())\n",
        "\n",
        "conv1.add(tf.keras.layers.MaxPool2D(pool_size=(1,2),strides=(2,2))) #output size (None, 1, 64, 512)\n",
        "conv1.add(tf.keras.layers.MaxPool2D(pool_size=(1,2),strides=(2,2))) #output size (None, 1, 32, 512)\n",
        "\n",
        "conv1.add(tf.keras.layers.Conv2D(filters=16, kernel_size=(1, 1), strides=1, padding='same', activation='linear')) #input size (None, 1, 32, 512)\n",
        "conv1.add(tf.keras.layers.BatchNormalization(axis=3))\n",
        "conv1.add(tf.keras.layers.ReLU())\n",
        "\n",
        "conv1.add(tf.keras.layers.Conv2D(filters=64, kernel_size=(1, 3), strides=1, padding='same', activation='linear')) #input size (None, 1, 32, 512)\n",
        "conv1.add(tf.keras.layers.BatchNormalization(axis=3))\n",
        "conv1.add(tf.keras.layers.ReLU())#input size (None, 1, 32, 64)\n",
        "\n",
        "\n",
        "\n",
        "encoded_left = conv1(shot_1)\n",
        "print(conv1.summary())\n",
        "\n",
        "conv2 = tf.keras.Sequential()\n",
        "conv2.add(tf.keras.layers.Reshape((1, placeSize, 4)))\n",
        "\n",
        "conv2.add(tf.keras.layers.Conv2D(filters=64, kernel_size=(1, 3), strides=1, padding='same', activation='linear')) #input size (None, 1, 2048, 4)\n",
        "conv2.add(tf.keras.layers.BatchNormalization(axis=3))\n",
        "conv2.add(tf.keras.layers.ReLU())\n",
        "\n",
        "conv2.add(tf.keras.layers.Conv2D(filters=64, kernel_size=(1, 3), strides=1, padding='same', activation='linear')) #input size (None, 1, 2048, 64)\n",
        "conv2.add(tf.keras.layers.BatchNormalization(axis=3))\n",
        "conv2.add(tf.keras.layers.ReLU())\n",
        "\n",
        "conv2.add(tf.keras.layers.MaxPool2D(pool_size=(1,2),strides=(2,2)))  #input size(none,1,2048,64)\n",
        "\n",
        "\n",
        "conv2.add(tf.keras.layers.Conv2D(filters=128, kernel_size=(1, 3), strides=1, padding='same', activation='linear')) #input size (None, 1, 1024, 64)\n",
        "conv2.add(tf.keras.layers.BatchNormalization(axis=3))\n",
        "conv2.add(tf.keras.layers.ReLU())\n",
        "\n",
        "conv2.add(tf.keras.layers.Conv2D(filters=128, kernel_size=(1, 3), strides=1, padding='same', activation='linear')) #input size (None, 1, 1024,64)\n",
        "conv2.add(tf.keras.layers.BatchNormalization(axis=3))\n",
        "conv2.add(tf.keras.layers.ReLU())\n",
        "\n",
        "conv2.add(tf.keras.layers.MaxPool2D(pool_size=(1,2),strides=(2,2)))   #input size(none,1,512,128)\n",
        "\n",
        "\n",
        "conv2.add(tf.keras.layers.Conv2D(filters=256, kernel_size=(1, 3), strides=1, padding='same', activation='linear')) #input size (None, 1, 512, 128)\n",
        "conv2.add(tf.keras.layers.BatchNormalization(axis=3))\n",
        "conv2.add(tf.keras.layers.ReLU())\n",
        "\n",
        "conv2.add(tf.keras.layers.Conv2D(filters=256, kernel_size=(1, 3), strides=1, padding='same', activation='linear')) #input size (None, 1, 512, 128)\n",
        "conv2.add(tf.keras.layers.BatchNormalization(axis=3))\n",
        "conv2.add(tf.keras.layers.ReLU())\n",
        "\n",
        "conv2.add(tf.keras.layers.Conv2D(filters=256, kernel_size=(1, 3), strides=1, padding='same', activation='linear')) #input size (None, 1, 512, 128)\n",
        "conv2.add(tf.keras.layers.BatchNormalization(axis=3))\n",
        "conv2.add(tf.keras.layers.ReLU())\n",
        "\n",
        "conv2.add(tf.keras.layers.MaxPool2D(pool_size=(1,2),strides=(2,2))) #output size (None, 1, 256, 256)\n",
        "\n",
        "conv2.add(tf.keras.layers.Conv2D(filters=512, kernel_size=(1, 3), strides=1, padding='same', activation='linear')) #input size (None, 1, 256, 256)\n",
        "conv2.add(tf.keras.layers.BatchNormalization(axis=3))\n",
        "conv2.add(tf.keras.layers.ReLU())\n",
        "\n",
        "\n",
        "conv2.add(tf.keras.layers.Conv2D(filters=512, kernel_size=(1, 3), strides=1, padding='same', activation='linear')) #input size (None, 1, 256, 256)\n",
        "conv2.add(tf.keras.layers.BatchNormalization(axis=3))\n",
        "conv2.add(tf.keras.layers.ReLU())\n",
        "\n",
        "conv2.add(tf.keras.layers.Conv2D(filters=512, kernel_size=(1, 3), strides=1, padding='same', activation='linear')) #input size (None, 1, 256, 256)\n",
        "conv2.add(tf.keras.layers.BatchNormalization(axis=3))\n",
        "conv2.add(tf.keras.layers.ReLU())\n",
        "\n",
        "conv2.add(tf.keras.layers.MaxPool2D(pool_size=(1,2),strides=(2,2))) #output size (None, 1, 128, 512)\n",
        "\n",
        "conv2.add(tf.keras.layers.Conv2D(filters=512, kernel_size=(1, 3), strides=1, padding='same', activation='linear')) #input size (None, 1, 128, 512)\n",
        "conv2.add(tf.keras.layers.BatchNormalization(axis=3))\n",
        "conv2.add(tf.keras.layers.ReLU())\n",
        "\n",
        "\n",
        "conv2.add(tf.keras.layers.Conv2D(filters=512, kernel_size=(1, 3), strides=1, padding='same', activation='linear')) #input size (None, 1, 128, 512)\n",
        "conv2.add(tf.keras.layers.BatchNormalization(axis=3))\n",
        "conv2.add(tf.keras.layers.ReLU())\n",
        "\n",
        "conv2.add(tf.keras.layers.Conv2D(filters=512, kernel_size=(1, 3), strides=1, padding='same', activation='linear')) #input size (None, 1, 128, 512)\n",
        "conv2.add(tf.keras.layers.BatchNormalization(axis=3))\n",
        "conv2.add(tf.keras.layers.ReLU())\n",
        "\n",
        "conv2.add(tf.keras.layers.MaxPool2D(pool_size=(1,2),strides=(2,2))) #output size (None, 1, 64, 512)\n",
        "conv2.add(tf.keras.layers.MaxPool2D(pool_size=(1,2),strides=(2,2))) #output size (None, 1, 32, 512)\n",
        "\n",
        "conv2.add(tf.keras.layers.Conv2D(filters=16, kernel_size=(1, 1), strides=1, padding='same', activation='linear')) #input size (None, 1, 32, 512)\n",
        "conv2.add(tf.keras.layers.BatchNormalization(axis=3))\n",
        "conv2.add(tf.keras.layers.ReLU())\n",
        "\n",
        "conv2.add(tf.keras.layers.Conv2D(filters=64, kernel_size=(1, 3), strides=1, padding='same', activation='linear')) #input size (None, 1, 32, 512)\n",
        "conv2.add(tf.keras.layers.BatchNormalization(axis=3))\n",
        "conv2.add(tf.keras.layers.ReLU())#input size (None, 1, 32, 64)\n",
        "\n",
        "encoded_right = conv2(shot_2)\n",
        "print(conv2.summary())\n",
        "\n",
        "combine = tf.keras.layers.concatenate([encoded_left, encoded_right], axis = 2)\n",
        "flat = tf.keras.layers.Flatten()(combine) #output size (None, 65536)\n",
        "d1 = tf.keras.layers.Dropout(rate=0.2)(flat)\n",
        "d2 = tf.keras.layers.Dense(4096, activation='relu')(d1)\n",
        "d2r = tf.keras.layers.Dropout(rate=0.2)(d2)\n",
        "d3 = tf.keras.layers.Dense(2048, activation='relu')(d2r)\n",
        "d3r = tf.keras.layers.Dropout(rate=0.2)(d3)\n",
        "output = tf.keras.layers.Dense(2, activation = 'softmax')(d3r)\n",
        "\n",
        "model = tf.keras.Model(inputs = inputs, outputs = output)\n",
        "opt = tf.keras.optimizers.SGD(learning_rate=0.02)\n",
        "\n",
        "model.compile(optimizer = opt, loss = BinaryFocalLoss(pos_weight = 70, gamma=1.5), metrics=[tf.keras.metrics.Accuracy()])\n",
        "model.summary()\n",
        "\n",
        "training_log = 'softmax_1_final' + '.txt'\n",
        "csv_logger = tf.keras.callbacks.CSVLogger(training_log, append = True, separator=' ')\n",
        "metrics = model.fit(X, tf.one_hot(YTruth, depth=2), epochs=16, validation_split= 0.2, verbose=2, batch_size = 32)\n",
        "\n",
        "model_ID = folder +  '/' + 'softmax_1' + '.h5'\n",
        "print(model_ID)\n",
        "tf.keras.models.save_model(model,model_ID)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8ecwp4kmh6Un"
      },
      "outputs": [],
      "source": [
        "tf.keras.utils.plot_model(model, to_file='cnn_VGG.png', dpi=100)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GaYMReutTpkd"
      },
      "outputs": [],
      "source": [
        "folder='/content/gdrive/MyDrive/movies/data/testing1'\n",
        "files=os.listdir(folder)\n",
        "noOfFiles = len(files) \n",
        "shotsforMovie = np.zeros((noOfFiles), dtype = np.uint16)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I8ei9tzsJrbg"
      },
      "outputs": [],
      "source": [
        "#initializing the variables needed\n",
        "k=0\n",
        "j=0\n",
        "counter=0\n",
        "i=0\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gIJfuZnYJjnj"
      },
      "outputs": [],
      "source": [
        "for i in files:\n",
        "  filename=folder+'/'+i\n",
        "  with open(filename,'rb') as f:\n",
        "     data=pickle.load(f)\n",
        "  \n",
        "  #converting tensors to arrays for features\n",
        "  place=data['place']\n",
        "  place=place.data.numpy()\n",
        "  placeSize=place.shape[1]\n",
        "\n",
        "  cast=data['cast']\n",
        "  cast=cast.data.numpy()\n",
        "  castSize=cast.shape[1]\n",
        "\n",
        "  action=data['action']\n",
        "  action=action.data.numpy()\n",
        "  actionSize=action.shape[1]\n",
        "\n",
        "  audio=data['audio']\n",
        "  audio=audio.data.numpy()\n",
        "  audioSize=audio.shape[1]\n",
        "\n",
        "  #combining all the features into a huge table of 1101 rows(no.of shots) and 3584 columns(2048+512+512+512)\n",
        "  x = np.hstack((place, cast, action, audio))\n",
        "  y = data['scene_transition_boundary_ground_truth']\n",
        " \n",
        "  N = x.shape[0] \n",
        "  x_fold = np.zeros((N - 1, 2*x.shape[1])) \n",
        "\n",
        "  #adding the adjoining shots to abstract features from it.\n",
        "  for i in range(N - 1):\n",
        "    x_fold[i,:] = np.hstack((x[i,:], x[i+1,:])) \n",
        "    \n",
        "  if (j == 0):\n",
        "    X = x_fold\n",
        "    Y = y\n",
        "\n",
        "  else:\n",
        "    X = np.concatenate((X, x_fold), axis = 0)\n",
        "    Y = np.concatenate((Y, y))\n",
        "    \n",
        "\n",
        "    \n",
        "  j = j + 1\n",
        "  k = k + 1\n",
        "\n",
        "print(X.shape)\n",
        "print(Y.shape)\n",
        "\n",
        "#converting the predictions to 0 and 1 from false and true\n",
        "M = Y.shape[0]\n",
        "YTruth = np.zeros((M), dtype = np.uint8)\n",
        "for i in range(M):\n",
        "  if(Y[i] == True):\n",
        "      YTruth[i] = 1\n",
        "  elif(Y[i] == False):\n",
        "      YTruth[i] = 0\n",
        "\n",
        "metrics2=model.evaluate(X,tf.one_hot(YTruth, depth=2))\n",
        "metrics2"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "SceneSegmentation.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMaMruEGFVDPY+fu95pmA+S",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}